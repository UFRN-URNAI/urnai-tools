import os

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

from .abneuralnetwork import ABNeuralNetwork
from .sequential_lambda import SequentialLambda


class KerasDeepNeuralNetwork(ABNeuralNetwork):
    """
    Implementation of a Generic Deep Neural Network using Keras.

    This class inherits from ABNeuralNetwork, so it already has all abstract methods
    necessary for learning, predicting outputs, and building a model. All that this
    class does is implement those abstract methods, and implement the methods necessary
    for adding Neural Network layers, such as add_input_layer(), add_output_layer(),
    add_fully_connected_layer(), add_convolutional_layer() etc.

    This class also implements the methodes necessary for saving and loading the model
    from local memory.

    Parameters:
        action_output_size: int
            size of our output
        state_input_shape: tuple
            shape of our input
        build_model: Python dict
            A dict representing the NN's layers. Can be generated by the
            ModelBuilder.get_model_layout() method from an instantiated ModelBuilder object.
        gamma: Float
            Gamma parameter for the Deep Q Learning algorithm
        alpha: Float
            This is the Learning Rate of the model
        seed: Integer (default None)
            Value to assing to random number generators in Python and our ML libraries to try
            and create reproducible experiments
        batch_size: Integer
            Size of our learning batch to be passed to the Machine Learning library
    """

    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed=None,
                 batch_size=32, run_eagerly=False):
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed,
                         batch_size)

        self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'],
                           run_eagerly=run_eagerly)

    def add_input_layer(self, idx):
        # self.model.add(Input(shape=self.build_model[idx]['shape']))
        self.model.add(
            Dense(self.build_model[idx]['nodes'], input_dim=self.build_model[idx]['shape'],
                  activation='relu'))

    def add_output_layer(self, idx):
        self.model.add(Dense(self.action_output_size, activation='linear'))

    def add_fully_connected_layer(self, idx):
        self.model.add(Dense(self.build_model[idx]['nodes'], activation='relu'))

    def add_convolutional_layer(self, idx):
        # if idx is zero it means this is an input layer, so we pass input_shape to keras
        if idx == 0:
            self.model.add(
                Conv2D(
                    filters=self.build_model[idx]['filters'],
                    kernel_size=self.build_model[idx]['filter_shape'],
                    activation=self.build_model[idx]['activation'],
                    padding=self.build_model[idx]['padding'],
                    strides=self.build_model[idx]['strides'],
                    data_format=self.build_model[idx]['data_format'],
                    dilation_rate=self.build_model[idx]['dilation_rate'],
                    groups=self.build_model[idx]['groups'],
                    input_shape=self.build_model[idx]['input_shape'],
                ),
            )
        else:
            self.model.add(
                Conv2D(
                    filters=self.build_model[idx]['filters'],
                    kernel_size=self.build_model[idx]['filter_shape'],
                    activation=self.build_model[idx]['activation'],
                    padding=self.build_model[idx]['padding'],
                    strides=self.build_model[idx]['strides'],
                    data_format=self.build_model[idx]['data_format'],
                    dilation_rate=self.build_model[idx]['dilation_rate'],
                    groups=self.build_model[idx]['groups'],
                ),
            )

    def add_maxpooling_layer(self, idx):
        self.model.add(
            MaxPooling2D(
                pool_size=self.build_model[idx]['pool_size'],
                strides=self.build_model[idx]['strides'],
                padding=self.build_model[idx]['padding'],
                data_format=self.build_model[idx]['data_format'],
            ),
        )

    def add_flatten_layer(self, idx):
        self.model.add(Flatten())

    def update(self, state, target_output):
        loss = self.model.fit(state, target_output, batch_size=self.batch_size, shuffle=False, verbose=0)

    def get_output(self, state):
        # return self.model.predict(state)
        return self.model(state).numpy()

    def set_seed(self, seed) -> None:
        if seed is not None:
            tf.random.set_seed(seed)
        return seed

    def create_base_model(self):
        model = Sequential()
        return model

    def save_extra(self, persist_path):
        self.model.save_weights(self.get_full_persistance_path(persist_path) + '.h5')

    def load_extra(self, persist_path):
        exists = os.path.isfile(self.get_full_persistance_path(persist_path) + '.h5')

        if exists:
            self.__init__(self.action_output_size, self.state_input_shape, self.build_model,
                          self.gamma, self.alpha, self.seed, self.batch_size)
            self.model.load_weights(self.get_full_persistance_path(persist_path) + '.h5')

    def copy_model_weights(self, model_to_copy):
        self.model.set_weights(model_to_copy.model.get_weights())


class DNNCustomModelOverrideExample(KerasDeepNeuralNetwork):
    """
    This Class is a custom handcrafted Keras Neural Network.

    To create a custom model we override the make_model() method from
    tensorflow.kerasDeepNeuralNetwork,
    and then define our model architecture.

    This sort of inheritance and override is very useful for advanced users of Keras, since it
    lets them
    customize a model to have whichever Neural Network architecture they desire, and then be
    assured that
    this model will still work like any other out-of-the-box URNAI models
    (KerasDeepNeuralNetwork for example),
    saving itself to disk, and supporting the automatic generation of graphs.

    In order to use this class, we just need to pass it as the 'neural_net_class' argument in
    one of our
    Reinforcement Learning algorithms, such as DeepQLearning (urnai.models.algorithms.dql) or
    DoubleDeepQLearning (urnai.models.algorithms.ddql).
    """

    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed=None,
                 batch_size=32):
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed,
                         batch_size)

    def make_model(self):
        self.model = Sequential()

        self.model.add(Dense(50, activation='relu', input_dim=self.state_input_shape))
        # self.model.add(Dense(50, activation='relu'))
        self.model.add(Dense(self.action_output_size, activation='linear'))

        self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'])


class KerasDNNEligibilityTraces(KerasDeepNeuralNetwork):
    """
    This class acts exactly like KerasDeepNeuralNetwork but instead
    of using the default Sequential() keras class, we use a custom
    urnai class that inherits from Sequential() called 'SequentialLambda()'.

    SequentialLambda implements the logic for updating and using a vector
    of eligibility traces during training.
    """

    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed=None,
                 batch_size=32, run_eagerly=True, lamb=0.9):
        self.lamb = lamb
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed,
                         batch_size, run_eagerly)

    def create_base_model(self):
        model = SequentialLambda(self.gamma, self.lamb)
        return model

    def reset_e_trace(self):
        self.model.reset_e_trace()

class KerasMultiGPUExample(KerasDeepNeuralNetwork):
    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed=None,
                 batch_size=32):
        self.strategy = tf.distribute.MirroredStrategy()
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed,
                            batch_size*self.strategy.num_replicas_in_sync)
    
    def make_model(self):
        with self.strategy.scope():
            self.model = Sequential()

            self.model.add(Dense(84, activation='relu', input_dim=self.state_input_shape))
            self.model.add(Dense(self.action_output_size, activation='relu'))

            self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'])
    
    def update(self, state, target_output):
        self.model.fit(state, target_output, shuffle=False, verbose=0)

    def get_model_layout(self):
        return self.model

class KerasCNN(KerasDeepNeuralNetwork):
    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed=None,
                 batch_size=32):
        self.strategy = tf.distribute.MirroredStrategy()
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed,
                            batch_size*self.strategy.num_replicas_in_sync)

    def make_model(self):
        with self.strategy.scope():
            self.model = Sequential()

            self.model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(64,64,3)))
            self.model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='same'))
            self.model.add(Conv2D(64, (3,3), activation='relu', padding='same'))
            self.model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='same'))
            self.model.add(Flatten())
            self.model.add(Dense(84, activation='relu'))
            self.model.add(Dense(84, activation='relu'))
            self.model.add(Dense(self.action_output_size, activation='relu'))

            self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'])

    def get_model_layout(self):
        return self.model